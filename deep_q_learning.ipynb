{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym version:[0.21.0]\n",
      "TF:[2.7.0]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "print (\"gym version:[%s]\"%(gym.__version__))\n",
    "print (\"TF:[%s]\"%(tf.__version__))\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_animation(anim):\n",
    "    plt.close(anim._fig)\n",
    "    return HTML(anim.to_jshtml())\n",
    "def display_frames_as_gif(frames):\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "    anim = animation.FuncAnimation(\n",
    "        plt.gcf(),animate,frames=len(frames),interval=30)\n",
    "    display(display_animation(anim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP Q LEARNING AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, obs_dim, n_action, seed=0,\n",
    "                 discount_factor = 0.995, epsilon_decay = 0.999, epsilon_min = 0.01,\n",
    "                 learning_rate = 1e-3, # Step size for Adam\n",
    "                 batch_size = 64, \n",
    "                 memory_size = 2000, hidden_unit_size = 64):\n",
    "        \n",
    "        self.seed = seed \n",
    "        \n",
    "        # Environment Information\n",
    "        self.obs_dim = obs_dim\n",
    "        self.n_action = n_action\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        # Epsilon Greedy Policy\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # Network Hyperparameters\n",
    "        self.hidden_unit_size = hidden_unit_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.train_start = 1000\n",
    "\n",
    "        # Experience Replay\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "        # Define Computational Graph in TF\\\n",
    "        self.build_model()\n",
    "    \n",
    "    def build_model(self): # Build networks\n",
    "        hid1_size = self.hidden_unit_size\n",
    "        hid2_size = self.hidden_unit_size\n",
    "        \n",
    "        q_prediction = tf.keras.Sequential()\n",
    "        q_prediction.add(tf.keras.Input(shape=(self.obs_dim,)))\n",
    "        q_prediction.add(tf.keras.layers.Dense(hid1_size,activation='relu',kernel_initializer='he_uniform'))\n",
    "        q_prediction.add(tf.keras.layers.Dense(hid2_size,activation='relu',kernel_initializer='he_uniform'))\n",
    "        q_prediction.add(tf.keras.layers.Dense(self.n_action, activation='linear'))\n",
    "        q_prediction.build()\n",
    "        \n",
    "        q_target = tf.keras.Sequential()\n",
    "        q_target.add(tf.keras.Input(shape=(self.obs_dim,)))\n",
    "        q_target.add(tf.keras.layers.Dense(hid1_size,activation='relu',kernel_initializer='he_uniform'))\n",
    "        q_target.add(tf.keras.layers.Dense(hid2_size,activation='relu',kernel_initializer='he_uniform'))\n",
    "        q_target.add(tf.keras.layers.Dense(self.n_action, activation='linear'))\n",
    "        q_target.build()\n",
    "        \n",
    "        q_target.set_weights(q_prediction.get_weights())\n",
    "        \n",
    "        self.q_prediction = q_prediction\n",
    "        self.q_target = q_target\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate,beta_1=0.9,beta_2=0.999,epsilon=1e-07)\n",
    "        \n",
    "    def update_target(self): # Update parameters\n",
    "        self.q_target.set_weights(self.q_prediction.get_weights())\n",
    "                \n",
    "    def update_policy(self):\n",
    "        if self.epsilon > self.epsilon_min: # Update epsilon\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "    def get_prediction(self, obs): # Get Q value from prediction network\n",
    "        q_value=self.q_prediction(obs)\n",
    "        return q_value\n",
    "\n",
    "    def get_action(self, obs): # Epsilon Greedy policy\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.n_action)\n",
    "        else:\n",
    "            q_value = self.get_prediction(obs)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    def add_experience(self, obs, action, reward, next_obs, done): # Add experience to memory\n",
    "        self.memory.append((obs, action, reward, next_obs, done))\n",
    "\n",
    "    def train_model(self):\n",
    "        loss = np.nan\n",
    "        n_entries = len(self.memory)\n",
    "            \n",
    "        if n_entries > self.train_start: # Start training when the number of experience is greater than batch size\n",
    "            \n",
    "            # Randomly sample batch\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            \n",
    "            observations = np.zeros((self.batch_size, self.obs_dim))\n",
    "            next_observations = np.zeros((self.batch_size, self.obs_dim))\n",
    "            actions = np.zeros((self.batch_size, ))\n",
    "            rewards = np.zeros((self.batch_size, ))\n",
    "            dones = np.zeros((self.batch_size, ))\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                observations[i] = mini_batch[i][0]\n",
    "                actions[i] = mini_batch[i][1]\n",
    "                rewards[i] = mini_batch[i][2]\n",
    "                next_observations[i] = mini_batch[i][3]\n",
    "                dones[i] = mini_batch[i][4]\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                next_q = self.q_target(next_observations)\n",
    "                max_next_q = tf.reduce_max(next_q, axis=1)\n",
    "                td_target = rewards + self.discount_factor*max_next_q*(1.0-dones) # R + gamma*max(Q)\n",
    "                q_est = tf.reduce_sum(\n",
    "                    self.q_prediction(observations) * tf.one_hot(actions,self.n_action,1.0,0.0),\n",
    "                    axis=1)\n",
    "                loss = tf.keras.losses.mse(tf.stop_gradient(td_target),q_est)\n",
    "                \n",
    "            gradients = tape.gradient(loss, self.q_prediction.trainable_weights)\n",
    "            self.optimizer.apply_gradients(\n",
    "                zip(gradients,self.q_prediction.trainable_variables))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE ENVIRONMENT AND AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space\n",
      "<class 'gym.spaces.box.Box'>\n",
      "(4,)\n",
      "Dimension:4\n",
      "High: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "Low: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "\n",
      "Action space\n",
      "<class 'gym.spaces.discrete.Discrete'>\n",
      "Total 2 actions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "obs_space = env.observation_space\n",
    "print('Observation space')\n",
    "print(type(obs_space))\n",
    "print(obs_space.shape)\n",
    "print(\"Dimension:{}\".format(obs_space.shape[0]))\n",
    "print(\"High: {}\".format(obs_space.high))\n",
    "print(\"Low: {}\".format(obs_space.low))\n",
    "print()\n",
    "\n",
    "act_space = env.action_space\n",
    "print('Action space')\n",
    "print(type(act_space))\n",
    "print(\"Total {} actions\".format(act_space.n))\n",
    "print()\n",
    "\n",
    "env.seed(seed)\n",
    "max_t = env.spec.max_episode_steps\n",
    "agent = DQNAgent(env.observation_space.high.shape[0],env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/500] loss : 3653.745, return : 33.400, eps : 0.184\n",
      "[150/500] loss : 280.133, return : 150.000, eps : 0.010\n",
      "[200/500] loss : 12.430, return : 148.100, eps : 0.010\n",
      "[250/500] loss : 10488.397, return : 290.900, eps : 0.010\n",
      "[300/500] loss : 21269.902, return : 452.600, eps : 0.010\n",
      "[350/500] loss : 804.609, return : 207.100, eps : 0.010\n",
      "[361/500] loss : 6110.642, return : 500.000, eps : 0.010\n",
      "The problem is solved with 361 episodes\n"
     ]
    }
   ],
   "source": [
    "avg_return_list = deque(maxlen=10)\n",
    "avg_loss_list = deque(maxlen=10)\n",
    "nepisodes = 500\n",
    "for i in range(nepisodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    for t in range(max_t):\n",
    "        # Get transition\n",
    "        action = agent.get_action(obs.reshape(1,-1))\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Add experience\n",
    "        agent.add_experience(obs,action,reward,next_obs,done)\n",
    "        \n",
    "        # Online update perdiction network parameter\n",
    "        loss = agent.train_model()\n",
    "        agent.update_policy()\n",
    "                \n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        total_loss += loss\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Update target network parameter\n",
    "    agent.update_target()\n",
    "    avg_return_list.append(total_reward)\n",
    "    avg_loss_list.append(total_loss)\n",
    "    \n",
    "    if (np.mean(avg_return_list) > 490): # Threshold return to success cartpole\n",
    "        print('[{}/{}] loss : {:.3f}, return : {:.3f}, eps : {:.3f}'.format(i,nepisodes, np.mean(avg_loss_list), np.mean(avg_return_list), agent.epsilon))\n",
    "        print('The problem is solved with {} episodes'.format(i))\n",
    "        break\n",
    "    \n",
    "    if i > 99 and (i%50)==0:\n",
    "        print('[{}/{}] loss : {:.3f}, return : {:.3f}, eps : {:.3f}'.format(i,nepisodes, np.mean(avg_loss_list), np.mean(avg_return_list), agent.epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward : 500.00\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "obs = env.reset()\n",
    "total_reward = 0\n",
    "frames = []\n",
    "for t in range(10000):\n",
    "    # Render into buffer. \n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    action = agent.get_action(obs.reshape(1,-1))\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "env.close()\n",
    "print('Total Reward : %.2f'%total_reward)\n",
    "display_frames_as_gif(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
